import os
from langchain_ollama import OllamaEmbeddings
from langchain_community.llms import Ollama
from db import get_user_collection
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# LLM & åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
llm = Ollama(model=os.getenv("LLM_MODEL", "tundere-ai:Q4_K_M"), base_url="http://127.0.0.1:11434")
embedding = OllamaEmbeddings(model=os.getenv("TEXT_EMBEDDING_MODEL", "nomic-embed-text"))

def save_dialog_summary(user_id: str, dialogs: list):
    """ä¼šè©±å±¥æ­´ã‚’ LLM ã§è¦ç´„ã—ã€ChromaDB ã«ä¿å­˜"""
    # ğŸ” ChromaDB ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼å°‚ç”¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰
    collection = get_user_collection(user_id)

    # ğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¼šè©±å±¥æ­´ã‚’çµåˆã—ã¦è¦ç´„
    dialog_text = "\n".join([f"{d.role}: {d.content}" for d in dialogs])
    llm_input = f"ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã‚’è¦ç´„ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å¾´ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚\n\n{dialog_text}"
    
    print("ğŸ¤– [ãƒ­ã‚°] LLM ã«è¦ç´„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡...")
    summary_response = llm.invoke(llm_input)
    
    if not summary_response:
        print("âš ï¸ [ã‚¨ãƒ©ãƒ¼] è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return False

    summary_text = summary_response.strip()
    print(f"ğŸ“ [ãƒ­ã‚°] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å¾´è¦ç´„: {summary_text}")

    # ğŸ”¥ **ChromaDB ã«ä¿å­˜**
    collection.add_texts([summary_text], metadatas=[{"user_id": user_id, "type": "summary"}])
    
    print("âœ… [ãƒ­ã‚°] è¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ ChromaDB ã«ä¿å­˜ã—ã¾ã—ãŸ")
    return True

def generate_response(user_id: str, message: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ RAG ã§å›ç­”"""
    # ğŸ” ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’ ChromaDB ã‹ã‚‰å–å¾—
    collection = get_user_collection(user_id)
    query_results = collection.similarity_search(message, k=3)

    if not query_results:
        print("âš ï¸ [ã‚¨ãƒ©ãƒ¼] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å¾´æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        context_texts = "ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é–¢ã™ã‚‹æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    else:
        context_texts = "\n".join([doc.page_content for doc in query_results])

    # ğŸ¤– **LLM ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**
    llm_input = f"ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è€ƒæ…®ã—ã¦ã€é©åˆ‡ãªè¿”ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã€‘\n{context_texts}\n\nã€è³ªå•ã€‘\n{message}"
    response = llm.invoke(llm_input)

    if not response:
        print("âš ï¸ [ã‚¨ãƒ©ãƒ¼] AI ã®å›ç­”ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    print(f"ğŸ’¬ [ãƒ­ã‚°] AI ã®è¿”ç­”: {response.strip()}")
    return response.strip()
